# Kickoff Meeting

## Raw Transcript 

And that Fermi Lab, it's another national lab in Illinois. And then the last three years I've been here at the Berkeley Lab and I kind of switched gears. So now I do more, our group is called Applied Nuclear Physics. And so what that is, basically using radiation detectors or just using radiation for benefits in real life. So kind of half of that work is things like Homeland Security, where you're, or basically environmental safety. So that could be things like surveying radiation in the environment, or like, for example, after some kind of disaster or after a bomb detonation, you want to know like where's radiation. Or they could be like finding nuclear material that's being smuggled, things like that. And then we also do a lot of work for medical applications, which could be cancer therapies based on like external beam therapy with photons or protons. Or we're starting to do a lot more work with like radioactive drugs, where if you have cancer that's spread in your body, you can engineer a molecule that will like target, stick to cancer cells and bring with it radioactive isotopes that can emit radiation there and kill the cancer. So anyway, we build all sorts of basically different types of radiation cameras or detectors to work on all those kinds of problems. We do a lot of like coding and software work as part of that, but most of us are not really software engineers. So the coding practices aren't always the best. But we get the job done. And then some people actually have very good engineering skills, although that's not really my forte. 

Yeah, along those lines as well. We saw that you guys were in the project page that we had at least that it looked like there was already some work done with deep learning models and sort of using computer vision. So that sort of led into, especially with like the project of how much is there already at this point? Is it going to be something that we're expanding on or is it something straight from the beginning? 

Yeah, so I would say, I mean, We've had a project, this project going for about 3 years, and the core of the project, well, maybe I'll come back to that question a little bit later, but I can show you some slides that kind of introduce the problem we're trying to solve, some of the, and the work we've done towards it, and then what we thought you guys could help with. Okay, yeah, that'd be great. Let's see, I need. Permission to share slide, to share my screen. 

Yep, I think I just sent the permissions now. 

Okay. Yeah, so sorry, these, I'm just going to use some slides from a talk I gave in a different context. But, and I can share you guys the PDF so you don't have to rely on just the recording, but I'll kind of talk through the idea. It is a project we've had for three years, so a lot of work has already been done, at least. But I think in terms of the engineering terms, I'll just say, like, I would say most of the work we've done is more of a proof of concept prototype. And a lot of the work that I think would be interesting for you guys to take on is kind of making this a little more formalized into like a more stable computing workflow. Not necessarily, well, and then if, I mean, we didn't, let me just come back to that a little bit later. But I guess the idea for this project, there's kind of two important concepts. One concept is the sensor. So that's what we call these CCDs or charge coupled devices. I don't know if you've heard of these in other contexts, but this is essentially like the original digital camera, where it's a silicon chip that has this divided into very small pixels. And it works like a traditional camera. You kind of start an exposure. And when you stop the exposure, you basically loop through all the pixels and read out what's the charge value in each pixel, and then that's basically, then you form an image from that matrix of charge values. That's not how like smartphone cameras work anymore, but it's just kind of like the origin of that. Before, now they use CMOS sensors, not CCDs, but for very precise applications like astronomy, CCDs are still what's used because you can get to much lower noise. You can get to very, very fine pixels, very, very good resolution. And so this project is kind of taking these really high performance CCD sensors, or we call them scientific CCDs, really made for astronomy work and using them basically as radiation detectors, which is kind of like not really what they were intended for. But it turns out that they work really well this way. So I just have some images here. This one, this array is like this incredibly large camera for the dark energy spectroscopic instrument, which DAISY, which is a big project here at LBL. But basically each of these tiles is a different image sensor, a different CCD, and they all kind of work together. So that's. kind of a, that we're kind of borrow some electronics from this group. And I just show on this other side, this other image is from actually an application using CCDs in a dark matter search where you're basically waiting for dark matter particles to kind of pass through the sensor and just gently bump the silicon. And because these have such low noise, you can see just very gentle bumps like that just give a little bit of energy. But kind of the way, what's interesting about these is, I mean, typically these are trying to collect light. And what happens in silicon is, you know, when light interacts, it actually can liberate some electrons. that are otherwise kind of stuck in the silicon. And so it kind of basically produces charge. And so that's their design as light sensors that sense the charge. But if you have higher energy ionizing radiation going through, those also can liberate some charges and they kind of make a signal just like light does. So if you're an astronomer, this kind of thing is really annoying. So for example, there are muons, which are basically high mass, kind of like high mass electrons that come from the upper atmosphere produced by radiation from space. They give you these really straight tracks through your CCD. And if you were trying to use this for imaging from your telescope in astronomy, like this would basically be something you have to go through and cut out because it's a It's just kind of a background or noise in your image. And so this graphic is just showing several different types of radiation that can interact. So there's also alpha particles. I'm not sure if you've heard of that before, but that's from an alpha decay from heavy nuclei. That gives you, can see the color scale is basically the intensity of the charge. So alphas give you a ton of charge in this kind of circular pattern. You can have electrons that are produced when a gamma ray hits an electron in the silicon. And so you get this basically, it's called a Compton scatter. You get an electron with a significant amount of energy and the electrons don't take quite as straight a path as for example, a muon. So you get these kind of squiggly lines. And then if you have lower energy things, they kind of just look like they end up having shorter tracks, they cover fewer pixels, you get smaller clusters like this. So that's basically, that's kind of how we use CCDs as radiation sensors. Maybe I should stop and ask you guys, like, I don't know how much you know about physics or radiation. Have you heard of any of these kind of interactions before? 

I know throughout, just basic sort of physical science classes in high school and college, just learning about alpha decay, beta decay, that kind of stuff. Not very in-depth on that topic in particular. 

Yeah, same here. 

Okay. 

I have a question though, on this picture you're showing. Those are not like trajectories that the particles are following. So what can you tell us about this graphic? Because I can see the mu one, it's like a straight line, right? And then you have the alpha particle, and I see it's a red in the center, so it's 30 KV, something like that, right? But about the electron, why does it look like that? I'm curious. 

Yes, so it's kind of... a combination. It's funny you said it's not a trajectory. I mean, so in some, sometimes, so let's call each of these things like a cluster. So some of the clusters of pixels do represent the trajectory of the particle. So for example, the muon, I think is easiest to understand. You should think of like the CCD is like a thin chip, right? So you, if you imagine I don't know if you can see my hands still, but if you imagine a particle is passing through, the particle is moving in 3D and the sensor is like just a 2D plane. This track that the muon leaves is representing like the 2D, the projection of that track onto that 2D surface. So this actually is the trajectory of the muon, except And the electron is kind of like that too. So that's showing really a 3D path that's projected to the plane of the sensor, wherever it crosses the sensor. There's some other things that affect it that become super relevant for the alpha. So that's that. Actually, let me have a better image here. 

No, hold on. So these particles are directly interacting with the sensor chip, right, and not some gas or something that's above it? 

Yeah, these are direct interactions with the silicon. No, there's no other medium. 

In that sense, it would be more of like the energy intensity at that point that it's receiving. And then it's sort of the path that it follows. So it could be different intensity or different, I guess, position of it. 

Yes, exactly. So let me show you the side view. So this. This, these cartoons on the right are showing you, like, if you're looking at the, from the side of the CCD, so this is really the thin volume. So this is like, ordered like hundreds of microns thick, where it's going to be like centimeters in the other two axes. So this is showing like the side view of a pixel. What happens, any, you have kind of an electric field across this volume. And so if you make any charge here, whether it's from optical light or it's from ionizing radiation, that's going to float, be drifted by the electric field to this pixel structure on the surface. And so what the image then is, basically a record of all of the charge that was deposited below that pixel. And the intensity is kind of a measure of how much, it is a measure of how much charge was produced right under that pixel, which is sometimes related to the energy of the particle. And so for example, the alpha has actually much higher energy or much higher energy deposition than everything else. So it has much, that's why it's red on this scale and most things are blue. But The other kind of critical thing about this is on the bottom right of this, the bottom figure here, basically the charge spreads out as it drifts to the surface. So this is kind of a cartoon showing you have a gamma ray coming in. The gamma ray doesn't do anything until it happens to hit an electron at this point where this white circle is. Then it gives a bunch of energy to the electron. And this is imagining actually like a very small track. So imagine it just deposited a lot of charge here. It's not really making an electron track that travels. But even if you just make a point of charge, what happens is as it goes, drifts across the sensor, the charge spreads out. And so then these kind of side panels are showing you a cross-section of the discharge cloud as it goes towards the pixels, which in this case are on the bottom. And you can see like at the beginning, it's sort of dense, but the longer it drifts, it gets really spread out. And so that actually, so that has a big influence on what you actually see. So for example, you can see one side of the muon is much thicker than the other side. And that's because that charge was produced farther from the pixels. And so by the time it gets to the pixels, it's actually just spread out more in X&Y. So, and kind of you can see similar effects like that with the electron, although The electron takes just a very, the electron is like bouncing off all sorts of things inside the sensor. So it's constantly changing direction. So it has this much more irregular path, but you can try to get a sense of what depth the charge was produced at based on this width, basically how much it's spread out when it reaches the surface. The alphas make so much charge that they immediately spread out to a large radius. because kind of charge sort of repels itself. So yeah, it's kind of very long-winded answer to your question, but basically some aspects of it are really about the trajectory of the particles, so like the mu and the electron. The alpha really interacts at a point. So this is not showing you the alpha's trajectory, but just showing how the charge spreads out by the time it gets to the pixels. So does that kind of answer your question? 

Yeah, I think. 

This also answers another question that you guys had about what is included in the CCD output data. And so what we're looking at here is actually a photo and it is what type of file? It's a map file for like Python? Or is it just like a regular image file? 

Yeah, okay, so this one I would call this, actually is not even our image. This is This, so this is not the direct output, but it's kind of, it's a similar structure. So I'll show you a little bit more. Well, yeah, let me just jump to that. So, well, no, let's come. So I first I want to kind of tell you what we're using these for. So this is a kind of introduction to CCDs as radiation detectors. For our specific, we kind of care about this in general, but we also have a very specific application, and that is for detecting tritium. I don't know if you've heard of tritium before, but that is an isotope of hydrogen, where you, so hydrogen just means it has, it's just one proton. You can have different number of neutrons, and it's still hydrogen. So tritium is hydrogen with two extra neutrons. And you can also have hydrogen with one neutron, and that's called deuterium. But tritium is something we care about because if you're running a nuclear power plant, you are, you produce tritium. It's almost, it's basically impossible to avoid that. And you'll even kind of, it's very hard to keep tritium contained because it behaves like hydrogen, which is just very, very small. There's water everywhere. So tritium is something that basically always escapes nuclear plants. And so it's something we care about detecting, both from an environmental perspective, but also it's kind of, this started as sort of like a nuclear intelligence. project where you want, you might want, you want to know if someone is running nuclear power plants in secret, because maybe they're making materials you could use for a weapon. So the idea is you want to be able to detect tritium basically in the atmosphere and try to correlate that with nuclear activities that are happening nearby. But it's actually a very tricky thing to detect because it has a very, very low energy decay. And I'm realizing now maybe I should have kind of made more of an introduction to beta decay or basically radioactivity, but basically when tritium decays, you get a beta particle, which is just an electron, and you get a couple of neutrinos, and then you're left with this helium 3. And the easiest way to see this is to actually observe this beta particle that's emitted. And for most beta decays, that's a pretty high energy thing that's easy to easy to see. For tritium, it's a special case where the energy is super low. So I don't, I guess these units don't mean much to you guys, but this is the energy spectrum in KEV. the maximum you can get from the tritium decay is 18 keV, and the most, the peak energy is like a few keV, which is very, very low for electrons. So for example, this can only travel about, I don't know, like much less than a millimeter, a few 100 microns at the high end in silicon. And so basically, like most kinds of radiation detectors, if you have any kind of like casing for that detector or like just anything around the sensitive volume, the tritium will be stopped there and never make it to actually be detected. Or sorry, the beta ray will be stopped in like the casing for your detector. So you need a really special radiation detector to be able to see these. And basically the best way people have to do this now is they mix, they make like single use cocktails where you take a water sample, you mix with that, you want to know how much tritium is in the water sample, you mix it with a special material called a liquid scintillator, where the liquid scintillator, if any radiation kind of crosses bumps into molecules in the scintillator, it will convert the energy to light. And then you can see that energy, you can see the light with a light sensor. But basically to do that, you have to take your water sample, mail it to the lab, then they have to make this cocktail, then they have to count it, like just watch that cocktail for like a month. And then they finally know how much tritium is there. And it's kind of a huge painful logistical process where like if you wanted to just, kind of the dream is instead of that, we have a system where you can just kind of deploy it. doesn't depend on anything consumable and it can operate outside of like a very controlled lab. And so that's what we want to use the CCDs for. The CCD has a chance to do this because they're just, they're very, very sensitive and precise. So you can measure For a CCD, like this amount of charge is actually kind of a lot. Like 5KEV is a very clear signature. This is so KEV, so this is like 5,000 electron volts. And the noise, to give you a sense, is like 10s of electron volts. But in terms of signal to noise, this is like very comfortable for the CCD. And you can make, we make special CCDs that have very little material between, very little inactive material at the edge. So if you basically, you could have a beta ray just enter the surface without having to cross much, without getting stopped by anything in the way. So that's kind of the specific application we care about. This was part of this, kind of a graphic that shows this program called GRAIL, which was sort of the inspiration for this work, where the idea is to pose you want to know if there's nuclear reactors, you take these daily air samples, you prepare some sample of water, you deposit that as ice, like on or near a CCD like this one, and then you can kind of see in sort of real time the beta decays. if there are tritiums, you'll see better education from the tritium coming into the sensor. And then you can kind of do a different measurement every single day. And the idea is you want to know the tritium level every day. So that's kind of the spirit of this. I think our group, we care about CCDs in general beyond just this application, but that's just to make you aware of, like, that's kind of been our focus so far. So do you guys have any questions about that before we go back to like what's the data format? 

I just wanted to put something out is that you did actually discern the lower energy beta decays from tritium directly. And until you started doing that with the deep learning that you have accomplished, no one was observing this decay event directly, right? That's how I understand it, correct me if I'm wrong, but that's why this is actually Very important. 

Yeah. So I mean, we have, in terms of things we've done first, I mean, I would say we just do it a little bit better than people have done. But so. Yeah, I mean, we didn't invent this idea. We didn't even, like other people have done tritium measurements with CCDs. I think where the deep learning came in is, yeah, having better way to reject things that look like this, but are not from tritium. We've done that better than other people have been able to do. Yeah, that's true. And that was kind of, that was kind of the goal of our project was basically machine learning methods to tell the difference between true tritium signals and other background sources. 

Would other instances of beta decay have similar energy signatures like that? 

Yeah, so in general, no. Most of them, so a lot of beta decay is are kind of like hundreds of keV or even MEV. 

Oh, okay. 

Which are, so those, going back to this cartoon, basically this thing's called low energy candidates. That's what the tritium beta looks like. But this squiggly electron is also what it would look like a more conventional beta emitting isotope would give you signals like that. This would be like 500 keV instead of 5 keV. But then there's funny, like, okay, but then some of them are sort of intermediate. So carbon 14 is also a common beta emitter. That's about 50 keV. And so there's some overlap where like the smallest energy carbon events could look like tritium. With beta decay, it's always a spectrum because it's, yeah, again, it's kind of obscure physics stuff, but with three particles, you have different ways to share the energy. So there's, yeah, actually, sorry, there's one neutrino, one anti-neutrino, the helium and the beta. They can all split up the energy differently, so the beta will end up with the whole spectrum. Whereas with gamma decay, you just have a single particle coming out the gamma ray, and so that's always one energy. But anyway, so the carbon will have some, carbon 14 has some overlap with this. 

Can you see gamma events on the CCD? 

Yes. So, okay, let's look at what the CCD exposures actually look like. So this is an example exposure from our colleagues, this is one of the CCDs for the Dark Energy Survey. And if you do, it's kind of an exaggerated exposure to highlight the radiation, but this is a 30-minute exposure where you basically just collect data for 30 minutes, then at the end you see what you got. If you zoom, it's kind of hard to see it's small. So if you zoom into a small square here, you get this little inlet figure. All of these squiggly lines are different gamma ray interactions. We call them, so it's really Compton, it's really Compton scattering. Well, there's a couple ways. So when we say gamma interaction, it really means a gamma hit an electron and gave the electron a bunch of energy. So what you actually see are basically electron tracks. and we call them, they're usually made with Compton scattering, so we just call them Compton electrons, or like this. So maybe I'll back up and say that these sensors are very thin, like less than a millimeter of silicon. Almost all gamma rays will just go through the sensor, not interact at all, and just don't leave If any charge don't scatter, don't be completely invisible. And so this is what you see is kind of the small minority that happened to hit an electron and give it a lot of energy. And those give you these squiggly tracks. That's most of what you see in this image. You kind of can barely make it out, but like most all of the squiggly things are electrons. There's also some of these straight, very straight tracks. Those are from muons that come from cosmic rays. And the way we kind of think about the analysis is we, maybe you've thought about like segmentation is a computer vision problem. We basically segment this image into what we call a cluster is basically our analysis unit. And a cluster is just a group of pixels that are above a threshold based on the noise. So it's basically the connected string of pixels that have significant charge. And so the way the analysis works is we take an exposure, we do the segmentation analysis, which is nothing magic. It's not even machine learning. It's just like literally iterating through the image, finding connected clusters. And then we end up with basically a list of clusters. A cluster is just like a 2D matrix of pixel XY and in charge. And that's it. Then we kind of do different shape analyses on those clusters and try to classify it like, is this a tritium beta ray? Is this a muon? Is this a Compton electron? And so you can see on the right here, we kind of calculate some basic statistics. So they have like a cluster index from this image. There's a, it finds a total energy summing all of the pixels. This one is 450 keV. Basic things like number of pixels above threshold, this one is 161. We have statistics about the width in X&Y. So that's just basically the standard deviation of the charge projected to the x-axis or the y-axis, that's the sigmas, or the full width in X&Y. So yeah, that's kind of the analysis, or at least kind of the core of the analysis are these clusters. And this format, I mean, so the format is simple. It's actually an astronomy format that we use. It's just dot FITS, but it's really all it is. It's just an object that holds a matrix of charge values. There's not really, it's not that structured. And I can show you, there's basically, we use a pipe, you can either use astronomy software to just visualize it directly, But for the actual analysis, we just have, there's just an easy Python module you can import that, reads that file and gives you the matrix. So it's not much to it to just load the data. 

I read from the paper, this is the physics network or something package. I wrote it somewhere. 

Yeah, that's actually coming later. So that's, we use those packages, those packages are for classification. But yeah, I was just talking about just accessing the data. There's not much to it. It's a format called FITS, FITS. But yeah, so okay, this is the Compton electron, this is the muon, and kind of more relevant for the tritium. This is on this slide is a different exposure that had a tritium source placed next to the sensor. And if you zoom in, this time you still see a lot of the squigglies on the straight lines from gammas and from muons, but these little tiny dots, those are actually the tritium beta rays. And So what those end up looking like when you zoom in is like this one on the top right. This one had just 5 keV, but you can see it's pretty, well, you might, yeah, you don't have much reference, but like for being so low energy, this is actually very spread out. And for example, at 5 keV, the electron Each of these pixels is like, is 10 microns square. The electron itself will travel less than a micron. And so what you're seeing is the effect of the charge spreading out as it gets to the pixels. And basically the way the CCD is set up, going back to this cartoon, We have the pixels, say, on the bottom here. The top surface is what's prepared for the tritium. And so all of the tritium bit arrays interact at the very, very front surface. And that means by the time they get to the pixels, they've traveled across the entire sensor. They're spread out as much as possible. Like they're the most spread out thing you can ever have at that energy. And so that's why this is relatively large. Whereas if you compare that to the kind of main background, this is a Compton interaction that just happens to give very little energy. So previously I was showing you hundreds of KEV Compton. This is a Compton interaction with also 5 KEV. But this is more likely to happen not at the surface, but kind of, it's equally likely that happened anywhere in the sensor. So for example, if the tritium happens here, that Compton event was happening somewhere in the bulk of the sensor, in the center of the sensor. And so it's just doesn't have, it doesn't spread out as much by the time it reaches the pixel. So you get this tighter cluster. And that is kind of the key handle between the signal and the background is basically this shape, this width. And a lot of the work we've done, I mean, the work we've done so far, we started with very straightforward, like classical measures of the width. And you can actually do pretty well just saying, okay, using like, for example, the standard deviation of the charge. already, you can tell this one's 0.5, this one's 0.9. You can kind of cut, you can make a selection based on very simple variables like that. actually does even pretty well. But at some point, what gets tricky is kind of the boundary case where you might have a very soft Compton event near the surface where the tritium is, and it's very hard or impossible to see the difference. And so having this kind of deep learning methods are basically able to distinguish closer and closer to that surface, but basically find a little shape difference, maybe 5 microns down versus maybe 20 micron downs with a kind of classical method. 

So again, I can help you with that too. So when we talk about energy of a particle, we're talking really about its speed. And with its speed comes a probability of interaction with the medium. So a fast one, so in the Compton regime, is going to be going pretty quick. It's going to pass through more material before it likely interacts. But it could interact at the surface too. It's just statistically likely to happen later. Whereas the tritium beta is going slow. It's lower energy. So it's more like it has a broader cross-section of interaction, right? And so it's likely to hit at the surface and do that spreading effect. Am I right there? 

Yeah, I think, I mean, the other part is that the species of particle. So the Compton is made from a photon, which is electrically neutral. And so neutral particles are just less likely to interact anywhere. And so they kind of just keep going through and then there's a small chance they interact somewhere. Whereas the beta ray is charged, it's an electron. And so it just always interacts when it crosses the material. So it just like has, it just starts interacting from the first surface just because it's charged. 

Right, I keep forgetting that the Compton is actually a photon event that happens due to a collision. 

Exactly, Yeah, and so, okay, I've been talking for a long time. I wanted to mention, yeah, so I forgot we sent you guys the paper. So the paper is all about different methods to input images like this and classify tritium or not tritium. Maybe we can talk more in depth about that at a future meeting. But I think I wanted to mention kind of what we had in mind for this project. It wasn't necessary, I mean, it could be more development of those computer vision or classification techniques. But kind of, I would say that's more of a bonus. The shorter term, or kind of the first thing we wanted to tackle is, well, most of the work we've done so far is based on simulated data or data from, yeah, mostly simulated data. We are building a experimental system here, or I'd say we built it to actually take real CCD data. And this is what this looks like. It's kind of complicated. You have to host the sensor inside this vacuum chamber. It has to be cooled, very, very cold. It's like a little bit warmer than liquid nitrogen, so like 100 Kelvin, because that reduces the noise. And because it's because it has to be operated that cold, that's why it has to be in a vacuum or else you'd get water condensing and freezing on all the surfaces. So the hardware is kind of a pain. We've set up the hardware. We have this readout electronics that actually controls the charge transfer between the pixels and basically grabs all of the pixel data from the sensor. This is something that's not quite working yet, but we think it will be working soon. And so the kind of the vision for this is we have the CCD stand. We want to be basically constantly taking, well, for a variety of reasons, we want to just like take lots of data to collect large background samples, for example, to make better training data sets. We want to collect large tritium data sets for the same purpose to train these machine learning methods. But there's also a couple other detection tasks besides tritium that we're interested in. And we kind of just want a multi-purpose like display system where If we're just taking this data, we want some software that can kind of immediately look at the acquired data, do the segmentation into clusters, do the classification of the cluster, the pixel clusters, and kind of show you live like, okay, here's, we took data from, you know, 3 o'clock to 3.15. It was in this exposure and we found like 17 muons. And here's what they looked like. Here is the distribution of where they came from. Or we found like 50 Tritium candidates and we think these five are actually Tritium beta arrays, things like that. So kind of like a cool graphical interface where we have a computer monitor next to this and just to kind of have this like live demonstration that's always running. that we can show people that come into the lab and you can kind of even like browse live and do some basic analysis on the fly. So I think that was kind of the first thing we imagined for you guys. I think if that goes well, yeah, we have another interesting project that we haven't really made any progress on. which would basically be using a CCD as a different kind of, I don't, yeah, as a different type of gamma camera, where basically we want the task would be not really classification, but for example, taking something like this Compton electron and trying to estimate the direction that it came from, which is tricky because the direction changes kind of immediately. So I think this is another good machine learning task beside, in addition to the classification. So anyway, so it's a long way to say, I think, yeah, the first step, we want to just have some kind of cool framework to visualize what's going on in the test stand. If you guys like that and you want to do more like computer vision stuff, there's a couple different avenues we could go. We could do more of the tritium classification. We could do this kind of electron tracking. So I think it will depend, you know, how it goes this year and how much progress we make. But there's, anyway, there's different ways we could go from there. 

Sounds good. 

So yeah, I don't know. Yeah, go ahead. 

Yeah, I was just trying to recap everything you said. So right now, the first objective would be to, well, you guys are working on the data, on capturing it using the device we see there. And you want to show us how the particle contracts look like in images so we can work in sort of a user interface to display everything so that someone can interact with the data and I don't know, do something with it, which it's DVD, basically, to be defined. Okay. 

I was thinking along those lines as well. Would it be something where you would want to look at like historic data as well? I know you had said like a 15 minute increment. say this was this activity, would there be plans for like storage of this data? And where you could go back, say it was this 30 minutes ago, 45 minutes ago, you could look at like running averages, that sort of thing. 

Absolutely. Yeah, that would be awesome. Like basically, yeah, if you could use the graphical interface and do some kind of like historical plotting. would be great. Or say like, show me all the muons from the last six weeks or something like that. 

Yeah, you have some kind of overlay. Because I'm assuming this is going to be running 24-7. Just gathering data at that point. 

Exactly. 

Cool. 

Yeah, I love that. Really cool. 

So basically do what you guys are already doing, but make it more interactable and with a code base that's extensible and flexible and make it so that it can kind of do anything. And then just start adding features that are user friendly and things that would allow a user to make intuitive judgments and anything that's graphical and yeah, human interface stuff. And then One thing you said, Ryan, about now and in the future, this is a project that does have a cutoff date, and I'm not sure when that is exactly. I think it's in June. I think that's about right. And so, like, if you want to like design the whole thing so that it's it all happens at once. And we can then hope to take that. And if the code base is, commented and everything, then we can just start doing whatever we want with it after that. But I think it's enough time to really build something really cool. And it's kind of open-ended too. So there's a lot of creativity involved. 

Great. Yeah. 

Quick question with that as well. How long was it taking beforehand to sort of go through the simulation data and everything else? Was it something that would just, you would gather a set and then run it in the background and then sort of sift through it manually at that point based on the classifications that it made? 

Yeah, I mean, it's, the processing is not that complicated. So I mean, I'd say we do it manually. But I mean, it would kind of be like you would simulate several million clusters overnight, and then the next day you run some classification algorithm on that. And then, but It's not like a huge computational task once you, once you, the simulation is kind of intensive, but then the analysis, it's like you can get by like semi-manual. 

Okay, yeah, that's sort of what I was wondering with that. But since this will be gathering data on its own, it won't really need that computational time in that sense. It's something that's actively constantly generating information that you can look at. 

Yeah, And for example, the real sensor produces data at a much lower rate than the simulation. So the computational burden is not much. It's more of the interface. It's not, it's not like a huge computational challenge. 

Right, yeah. 

I had a quick question about kind of the GUI in general. You said it was pretty open-ended. I kind of just want to know if there's any like particular features or anything like that you kind of, that you have in mind for it? 

So for me, I think a cool like starting point, I want to be able to just easily display like event, I call these kind of event displays. So I want to be able to just like browse event displays easily. So I think that's kind of like the basic functionality would be some way to interact with the data and find this list of displays. 

So like filter them out, sort them by type, and then maybe you could... click each one and then have it bring up that display picture. Exactly. 

Yeah. 

And with that data already gathered, it'd be easy to sort by energy or number of pixels or anything along those lines. 

Yep. So I think that kind of the basic workflow would be like you take an exposure, you do some processing once, which would be cutting up the image into segments, running some basic analysis of each segment, running some like maybe some basic classification. And then you end up with some kind of library of clusters for that acquisition. And then the interface would be filtering out items from that library, not necessarily going back to the full image each time. And I'm sorry, I have to go. I have another meeting at 4:00, but if I can run over a little bit if there's something burning right now. 

I think we have a pretty good baseline right now. I just want to thank you guys for taking the time to walk us through all of this and explain a lot of these concepts to us. I know it's been about six years since I've heard Alpha Decay, so it was very informative. 

I have one last question though. Those visualizations, are those, there is a package from certain root. Were those made with root or? 

Great question. So I personally do use root for a lot of things. This one, the ones I showed you are actually not from root. Those are from Matplotlib, I think. Yeah. Have you used root before? 

No, but I was reading about it last night, actually, because I was researching what kind of what kind of software are used for this kind of visualization, and that popped up, and that's a C package, and they have Python bindings and all that kind of stuff, and I was... and I saw this and I saw some pictures in the root website that look similar to this. And that's why, actually I was browsing in the background looking at the images and I got curious, this looks like this. So yeah, that's why I was wondering. 

Yeah, no, yeah. I use, I spent a lot of time using root, but now people kind of make fun of you if you use root. because everybody's really switching to Python tools. But I mean, like you said, Root has a lot of good, you can just interact with Root through Python and never have to think about C. But yeah. 

Actually, I'm a fan of C, you know, so I wouldn't mind. 

That. Yeah. Okay. 

I think everything's been abstracted away at this point. It's all just Python to call C. Yeah, Cool. 

And I saw you guys had a list of questions. I was hoping to get to that, but I think I can just type responses in your e-mail. The most, I mean, some of the questions I actually don't really understand, but I think I'm just less familiar with some of the tools you mentioned. But Yeah, we can get into that next time. 

All right, perfect. Sounds good. 

Yeah, thanks for coming, guys. This is really cool. This is awesome. I feel like we got the right people. And for me, I'm here for guidance and for help however I can. I'm not a huge programmer either, but I'm pretty good with C and Arduino stuff and ARM. And so, and I love product development and human interface stuff. So I'll be here and you guys can communicate as you see fit and we'll just start moving forward slowly. 

All right. Thank you guys. 

I appreciate it. Thank you very much. Thanks. 

Take care. 

Bye. 


